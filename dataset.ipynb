{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# portfolio optimization\n",
    "- dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path constant\n",
    "data_root = \"./data/\"\n",
    "raw = \"raw\"\n",
    "# manual constants\n",
    "\n",
    "# 다룰 데이터의 시작 지점\n",
    "start_date = \"2006-01-01\"\n",
    "end_date = \"2021-12-31\"\n",
    "timelines = [\"9008\", \"0924\"]\n",
    "timelines_alt = \"0208\"\n",
    "window_size_years = 7\n",
    "step_size_years = 1\n",
    "\n",
    "lookback_T = 60\n",
    "\n",
    "data_split: List[int] = {\n",
    "    \"train\": 5,\n",
    "    \"val\": 1,\n",
    "    \"test\": 1}  # train, val, test\n",
    "\n",
    "# data processing option\n",
    "filter_date: bool = True \n",
    "filter_holidays: bool = True\n",
    "\n",
    "base_ticker: str = \"SPX\"\n",
    "interpolation_target: List[str] = [\"SPXHC\"]  # 몇몇 거래일의 정보가 소실되어 있음\n",
    "\n",
    "start_date: pd.Timestamp = pd.to_datetime(start_date)\n",
    "end_date: pd.Timestamp = pd.to_datetime(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code snippet\n",
    "## 영업일 filter\n",
    "def get_closed_days(start_date: pd.Timestamp, end_date: pd.Timestamp):\n",
    "    us_holidays_dict = holidays.XNYS(years=range(start_date.year, end_date.year+1))  # 2002년부터 2023년까지의 공휴일\n",
    "    us_holidays = pd.to_datetime(list(us_holidays_dict.keys()))\n",
    "    \n",
    "    all_dates = pd.date_range(start=start_date, end=end_date)\n",
    "    weekends = all_dates[all_dates.weekday >= 5]  # 토요일(5), 일요일(6)\n",
    "\n",
    "    # 공휴일과 주말 합치기\n",
    "    all_holidays = pd.concat([pd.Series(us_holidays), pd.Series(weekends)]).drop_duplicates().sort_values()\n",
    "    \n",
    "    return all_holidays\n",
    "\n",
    "def convert_volume(value):  # to handle Vol. Column\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if value.endswith('B'):\n",
    "            return float(value[:-1]) * 10**9\n",
    "        elif value.endswith('M'):\n",
    "            return float(value[:-1]) * 10**6\n",
    "        elif value.endswith('K'):\n",
    "            return float(value[:-1]) * 10**3\n",
    "        elif value == '':\n",
    "            return np.nan\n",
    "        else:\n",
    "            return float(value)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# hard coded values    \n",
    "def convert_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    numeric_cols = ['Price', 'Open', 'High', 'Low']\n",
    "    df[numeric_cols] = df[numeric_cols].map(lambda x: str(x).replace(',', '')).astype(float)\n",
    "\n",
    "    df[\"Vol.\"] = df[\"Vol.\"].apply(convert_volume)\n",
    "\n",
    "    df['Change %'] = df['Change %'].str.replace('%', '', regex=False).astype(float) * 0.01\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data Processing S&P 500\n",
      "Raw data Processing S&P 500 Information Technology\n",
      "Raw data Processing S&P 500 Telecom Services\n",
      "Raw data Processing S&P 500 Materials\n",
      "Raw data Processing S&P 500 Real Estate\n",
      "Raw data Processing S&P 500 Consumer Staples\n",
      "Raw data Processing S&P 500 Financials\n",
      "Raw data Processing S&P 500 Energy\n",
      "Raw data Processing S&P 500 Health Care\n",
      "Raw data Processing S&P 500 Consumer Discretionary\n",
      "Raw data Processing S&P 500 Industrials\n",
      "Raw data Processing S&P 500 Utilities\n",
      "Raw data Processing CBOE Volatility Index\n",
      "Missing value linear interpolation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1409192/1213196479.py:59: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  snp_indices_raw_data[key].fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# raw data processing\n",
    "snp_indices_raw = {\n",
    "    \"SPX\": \"S&P 500\",  # done\n",
    "    \"SPLRCT\": \"S&P 500 Information Technology\",  # done\n",
    "    \"SPLRCL\": \"S&P 500 Telecom Services\",  # done\n",
    "    \"SPLRCM\": \"S&P 500 Materials\",  # done\n",
    "    \"SPLRCREC\": \"S&P 500 Real Estate\",  # 20020101부터 사용가능  # done\n",
    "    \"SPLRCS\": \"S&P 500 Consumer Staples\",  # done\n",
    "    \"SPSY\": \"S&P 500 Financials\",  # done\n",
    "    \"SPNY\": \"S&P 500 Energy\",  # done\n",
    "    \"SPXHC\": \"S&P 500 Health Care\",  # done, # 중간에 정보 손실 interpolation 해버리기\n",
    "    \"SPLRCD\": \"S&P 500 Consumer Discretionary\",  # done\n",
    "    \"SPLRCI\": \"S&P 500 Industrials\",  # done\n",
    "    \"SPLRCU\": \"S&P 500 Utilities\",  # done\n",
    "    \"VIX\": \"CBOE Volatility Index\",  # done\n",
    "}  # {ticker: name} for investing.com\n",
    "\n",
    "snp_indices_raw_data = {}\n",
    "for key, value in snp_indices_raw.items():\n",
    "    print(f\"Raw data Processing {value}\")\n",
    "    temp_df_list = []\n",
    "    for _each_timeline in timelines:\n",
    "        # load csv\n",
    "        if _each_timeline == \"9008\" and key == \"SPLRCREC\":\n",
    "            each_timeline = timelines_alt\n",
    "        else:\n",
    "            each_timeline = _each_timeline\n",
    "        csv_path = os.path.join(data_root, raw, f\"{key}{each_timeline}.csv\")    \n",
    "        each_df = pd.read_csv(csv_path)\n",
    "        each_df[\"Date\"] = pd.to_datetime(each_df[\"Date\"])\n",
    "        \n",
    "        # specific date range        \n",
    "        if filter_date:\n",
    "            each_df = each_df[(each_df[\"Date\"] >= start_date) & (each_df[\"Date\"] <= end_date)]\n",
    "\n",
    "        temp_df_list.append(each_df)\n",
    "        \n",
    "    merged_df = pd.concat(temp_df_list)\n",
    "    merged_df = merged_df.sort_values(\"Date\", ascending=True)\n",
    "    \n",
    "    if filter_holidays:\n",
    "        us_holidays = get_closed_days(start_date, end_date)\n",
    "        # 2002년부터 2023년까지의 공휴일\n",
    "        merged_df = merged_df[~merged_df[\"Date\"].isin(us_holidays)]\n",
    "        \n",
    "    merged_df = convert_columns(merged_df)\n",
    "        \n",
    "    merged_df[\"T\"] = range(len(merged_df))\n",
    "    merged_df = merged_df.reset_index(drop=True).set_index(\"T\")\n",
    "    \n",
    "    snp_indices_raw_data[key] = merged_df\n",
    "\n",
    "if len(interpolation_target) > 0:\n",
    "    print(\"Missing value ffilll interpolation\")\n",
    "    full_date = snp_indices_raw_data[base_ticker][\"Date\"]\n",
    "    \n",
    "    for key in interpolation_target:\n",
    "        snp_indices_raw_data[key] = snp_indices_raw_data[key].set_index(\"Date\").reindex(full_date)\n",
    "        snp_indices_raw_data[key].ffill(method='ffill', inplace=True)\n",
    "        \n",
    "        snp_indices_raw_data[key][\"T\"] = range(len(snp_indices_raw_data[key]))\n",
    "        snp_indices_raw_data[key] = snp_indices_raw_data[key].reset_index(drop=False).set_index(\"T\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Change %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>1268.8</td>\n",
       "      <td>1248.3</td>\n",
       "      <td>1270.2</td>\n",
       "      <td>1245.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-04</td>\n",
       "      <td>1273.5</td>\n",
       "      <td>1268.8</td>\n",
       "      <td>1275.4</td>\n",
       "      <td>1267.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>1273.5</td>\n",
       "      <td>1273.5</td>\n",
       "      <td>1276.9</td>\n",
       "      <td>1270.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-06</td>\n",
       "      <td>1285.5</td>\n",
       "      <td>1273.5</td>\n",
       "      <td>1286.1</td>\n",
       "      <td>1273.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-09</td>\n",
       "      <td>1290.2</td>\n",
       "      <td>1285.5</td>\n",
       "      <td>1290.8</td>\n",
       "      <td>1284.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>4791.2</td>\n",
       "      <td>4734.0</td>\n",
       "      <td>4791.5</td>\n",
       "      <td>4734.0</td>\n",
       "      <td>1.450000e+09</td>\n",
       "      <td>0.0138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>4786.4</td>\n",
       "      <td>4795.5</td>\n",
       "      <td>4807.0</td>\n",
       "      <td>4780.0</td>\n",
       "      <td>1.370000e+09</td>\n",
       "      <td>-0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>4793.1</td>\n",
       "      <td>4788.6</td>\n",
       "      <td>4804.1</td>\n",
       "      <td>4778.1</td>\n",
       "      <td>1.250000e+09</td>\n",
       "      <td>0.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>4778.7</td>\n",
       "      <td>4794.2</td>\n",
       "      <td>4808.9</td>\n",
       "      <td>4775.3</td>\n",
       "      <td>1.300000e+09</td>\n",
       "      <td>-0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>4766.2</td>\n",
       "      <td>4775.2</td>\n",
       "      <td>4786.8</td>\n",
       "      <td>4765.8</td>\n",
       "      <td>1.220000e+09</td>\n",
       "      <td>-0.0026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4028 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   Price    Open    High     Low          Vol.  Change %\n",
       "T                                                                      \n",
       "0    2006-01-03  1268.8  1248.3  1270.2  1245.7           NaN    0.0164\n",
       "1    2006-01-04  1273.5  1268.8  1275.4  1267.7           NaN    0.0037\n",
       "2    2006-01-05  1273.5  1273.5  1276.9  1270.3           NaN    0.0000\n",
       "3    2006-01-06  1285.5  1273.5  1286.1  1273.5           NaN    0.0094\n",
       "4    2006-01-09  1290.2  1285.5  1290.8  1284.8           NaN    0.0037\n",
       "...         ...     ...     ...     ...     ...           ...       ...\n",
       "4023 2021-12-27  4791.2  4734.0  4791.5  4734.0  1.450000e+09    0.0138\n",
       "4024 2021-12-28  4786.4  4795.5  4807.0  4780.0  1.370000e+09   -0.0010\n",
       "4025 2021-12-29  4793.1  4788.6  4804.1  4778.1  1.250000e+09    0.0014\n",
       "4026 2021-12-30  4778.7  4794.2  4808.9  4775.3  1.300000e+09   -0.0030\n",
       "4027 2021-12-31  4766.2  4775.2  4786.8  4765.8  1.220000e+09   -0.0026\n",
       "\n",
       "[4028 rows x 7 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snp_indices_raw_data[\"SPX\"]# key # snp_indices_raw_data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2006\n",
      "num work date: 251\n",
      "Year 2007\n",
      "num work date: 251\n",
      "Year 2008\n",
      "num work date: 253\n",
      "Year 2009\n",
      "num work date: 252\n",
      "Year 2010\n",
      "num work date: 252\n",
      "Year 2011\n",
      "num work date: 252\n",
      "Year 2012\n",
      "num work date: 250\n",
      "Year 2013\n",
      "num work date: 252\n",
      "Year 2014\n",
      "num work date: 252\n",
      "Year 2015\n",
      "num work date: 252\n",
      "Year 2016\n",
      "num work date: 252\n",
      "Year 2017\n",
      "num work date: 251\n",
      "Year 2018\n",
      "num work date: 251\n",
      "Year 2019\n",
      "num work date: 252\n",
      "Year 2020\n",
      "num work date: 253\n",
      "Year 2021\n",
      "num work date: 252\n",
      "Year 2022\n",
      "num work date: 0\n"
     ]
    }
   ],
   "source": [
    "for each_year in range(2006, 2023):\n",
    "    print(f\"Year {each_year}\")\n",
    "    print(f'num work date: {len(snp_indices_raw_data[\"SPX\"][snp_indices_raw_data[\"SPX\"][\"Date\"].dt.year == each_year])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPX: 4028\n",
      "SPX has the same dates.\n",
      "SPLRCT: 4028\n",
      "SPLRCT has the same dates.\n",
      "SPLRCL: 4028\n",
      "SPLRCL has the same dates.\n",
      "SPLRCM: 4028\n",
      "SPLRCM has the same dates.\n",
      "SPLRCREC: 4028\n",
      "SPLRCREC has the same dates.\n",
      "SPLRCS: 4028\n",
      "SPLRCS has the same dates.\n",
      "SPSY: 4028\n",
      "SPSY has the same dates.\n",
      "SPNY: 4028\n",
      "SPNY has the same dates.\n",
      "SPXHC: 4028\n",
      "SPXHC has the same dates.\n",
      "SPLRCD: 4028\n",
      "SPLRCD has the same dates.\n",
      "SPLRCI: 4028\n",
      "SPLRCI has the same dates.\n",
      "SPLRCU: 4028\n",
      "SPLRCU has the same dates.\n",
      "VIX: 4028\n",
      "VIX has the same dates.\n",
      "SPX missing dates:\n",
      "[]\n",
      "SPLRCT missing dates:\n",
      "[]\n",
      "SPLRCL missing dates:\n",
      "[]\n",
      "SPLRCM missing dates:\n",
      "[]\n",
      "SPLRCREC missing dates:\n",
      "[]\n",
      "SPLRCS missing dates:\n",
      "[]\n",
      "SPSY missing dates:\n",
      "[]\n",
      "SPNY missing dates:\n",
      "[]\n",
      "SPLRCD missing dates:\n",
      "[]\n",
      "SPLRCI missing dates:\n",
      "[]\n",
      "SPLRCU missing dates:\n",
      "[]\n",
      "VIX missing dates:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "sanity check\n",
    "동일한 영업일에서 데이터가 형성되어있는 가를 확인하기 위함\n",
    "SPXHC ticker에 대해서만 일부 데이터 누락이 있어서 interpolation이 필요함\n",
    "기준은 SPX로 한다.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# overall sanity check\n",
    "all_dates = [df[\"Date\"] for df in snp_indices_raw_data.values()]\n",
    "reference_dates = all_dates[0]\n",
    "\n",
    "for key, dates in zip(snp_indices_raw_data.keys(), all_dates):\n",
    "    print(f\"{key}: {len(dates)}\")\n",
    "    if not dates.equals(reference_dates):\n",
    "        print(f\"{key} has different dates.\")\n",
    "    else:\n",
    "        print(f\"{key} has the same dates.\")\n",
    "\n",
    "### ticker_wise check\n",
    "search_target = \"short_date\" # \"short_date\"\n",
    "target_ticker = \"SPXHC\"\n",
    "short_date = snp_indices_raw_data[target_ticker][\"Date\"].to_list()\n",
    "\n",
    "missing_dates = {}\n",
    "for key, each_df in snp_indices_raw_data.items():\n",
    "    if key != target_ticker:\n",
    "        missing_dates[key] = [date for date in short_date if date not in each_df[\"Date\"].values]\n",
    "\n",
    "# 누락된 날짜 출력\n",
    "for key, dates in missing_dates.items():\n",
    "    print(f\"{key} missing dates:\")\n",
    "    print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deform_struct(folded_data: List[Dict[str, Dict[str, pd.DataFrame]]]) -> List[Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    \"\"\"\n",
    "    [{ticker: {train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame}}] -> [{train: {ticker: pd.DataFrame}, val: {ticker: pd.DataFrame}, test: {ticker: pd.DataFrame}}]\n",
    "    \"\"\"\n",
    "    new_structure = []\n",
    "    for each_fold in folded_data:\n",
    "        new_structure.append(transform_dict_structure(each_fold))\n",
    "    \n",
    "    return new_structure\n",
    "\n",
    "def split_window_by_year(window: pd.DataFrame, split: Dict[str, int]) -> Dict[str, pd.DataFrame]:\n",
    "    split_dfs = {}\n",
    "    start_year = window['Date'].dt.year.min()\n",
    "    \n",
    "    for key, years in split.items():\n",
    "        end_year = start_year + years\n",
    "        split_dfs[key] = window[(window['Date'].dt.year >= start_year) & (window['Date'].dt.year < end_year)]\n",
    "        start_year = end_year\n",
    "    \n",
    "    return split_dfs  # {train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame}\n",
    "\n",
    "def window_by_year(df_dict: Dict[str, pd.DataFrame], start_date: pd.Timestamp, window_size_years: int, split: Dict[str, int], is_last: bool = False, raw_end_date: Optional[pd.Timestamp]=None) -> Dict[str, pd.DataFrame]:\n",
    "    folded_dict_df: Dict[str, pd.DataFrame] = {}\n",
    "    \n",
    "    if is_last:\n",
    "        assert raw_end_date is not None, \"raw_end_date is required\"\n",
    "        for key, each_df in df_dict.items():\n",
    "            window_each_df = each_df[(each_df['Date'] >= start_date) & (each_df['Date'] < raw_end_date)]\n",
    "            window_df_split_dict = split_window_by_year(window_each_df, split)\n",
    "            folded_dict_df[key] = window_df_split_dict\n",
    "    else:\n",
    "        for key, each_df in df_dict.items():\n",
    "            window_end_date = start_date + pd.DateOffset(years=window_size_years)\n",
    "            window_each_df = each_df[(each_df['Date'] >= start_date) & (each_df['Date'] < window_end_date)]\n",
    "            window_df_split_dict = split_window_by_year(window_each_df, split)\n",
    "            folded_dict_df[key] = window_df_split_dict\n",
    "\n",
    "    return folded_dict_df  # {ticker: {train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame}}\n",
    "\n",
    "def transform_dict_structure(folded_data: Dict[str, Dict[str, pd.DataFrame]]) -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \n",
    "    # {ticker: {train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame}} -> {train: {ticker: pd.DataFrame}, val: {ticker: pd.DataFrame}, test: {ticker: pd.DataFrame}}\n",
    "    \n",
    "    new_structure = {}\n",
    "    \n",
    "    for ticker, splits in folded_data.items():\n",
    "        for split, df in splits.items():\n",
    "            if split not in new_structure:\n",
    "                new_structure[split] = {}\n",
    "            new_structure[split][ticker] = df\n",
    "    \n",
    "    return new_structure\n",
    "\n",
    "def sliding_window_by_date(df_dict: Dict[str, pd.DataFrame], window_size_years: int, split: Dict[str, int], step_size_years: int, default_key: str):\n",
    "    assert window_size_years == sum(split.values()), \"Window size and split size mismatch\"\n",
    "    \n",
    "    folded_snp_indices_raw_data: List[Dict[Dict[str, Dict[str, pd.DataFrames]]]] = []\n",
    "    start_date = df_dict[default_key]['Date'].min()\n",
    "    end_date = df_dict[default_key]['Date'].max()\n",
    "    \n",
    "    while start_date + pd.DateOffset(years=window_size_years) <= end_date:\n",
    "        folded_dict_df = window_by_year(df_dict, start_date, window_size_years, split)\n",
    "        start_date += pd.DateOffset(years=step_size_years)\n",
    "        folded_snp_indices_raw_data.append(folded_dict_df)\n",
    "    \n",
    "    if start_date < end_date:\n",
    "        folded_dict_df = window_by_year(df_dict, start_date, window_size_years, split, is_last=True, raw_end_date=end_date)\n",
    "        folded_snp_indices_raw_data.append(folded_dict_df)\n",
    "        \n",
    "    return folded_snp_indices_raw_data\n",
    "\n",
    "# 슬라이딩 윈도우 적용\n",
    "windows_list_df_dict = sliding_window_by_date(snp_indices_raw_data, window_size_years, data_split, step_size_years, base_ticker)\n",
    "\n",
    "windows_list_df_dict_deformed = deform_struct(windows_list_df_dict)\n",
    "\n",
    "# 다만, train, eval, test split으로 분리를 해야 하니까 다시 조금만 더 가공이 필요함\n",
    "# TODO 그리고, 지금 걱정 되는 data 효율 처리를 위해서는 그냥 한 dataframe으로 만들어 버려야 겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment를 구축하기 전에 각각의 환경에 대해서 data prepocessing을 해야 한다.\n",
    "# neural network에 state로 들어가는 경우에 있어서는 log를 취한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elem_overall_processing(df_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1. 필요 없는 column drop, Price column만 남기기\n",
    "    2. 각 str를 price의 column name으로 변경하여 concatenate해버리기\n",
    "    3. return 추가하기 (R_t = (P_t - P_{t-1}) / P_{t-1}) R_1 = 1\n",
    "    4. log return 추가하기\n",
    "    5. vol_{20} 추가하기  # futher 이용\n",
    "    6. vol_{60} 추가하기\n",
    "    7. vol_{20} / vol_{60} 추가하기\n",
    "    8 5,6,7, VIX feature에 대해서 moving normalization 적용하기\n",
    "    \n",
    "    \"\"\"\n",
    "    new_df_list = []\n",
    "    for key, each_df in df_dict.items():\n",
    "        new_df = each_df[[\"Date\",\"Price\"]].copy()\n",
    "        new_df.rename(columns={\"Price\": f\"{key}_close\"}, inplace=True)\n",
    "        new_df_list.append(new_df)\n",
    "\n",
    "    merged_df = new_df_list[0]\n",
    "    for each_df in new_df_list[1:]:\n",
    "        merged_df = pd.merge(merged_df, each_df, on=\"Date\", how=\"outer\")\n",
    "        \n",
    "    for key in df_dict.keys():\n",
    "        if \"VIX\" in key:\n",
    "            continue\n",
    "        \n",
    "        # return 추가\n",
    "        merged_df[f'{key}_R'] = (merged_df[f\"{key}_close\"] - merged_df[f\"{key}_close\"].shift(1)) / merged_df[f\"{key}_close\"].shift(1)\n",
    "        merged_df.loc[0, f'{key}_R'] = 0.0\n",
    "        \n",
    "        # log return 추가\n",
    "        merged_df[f'{key}_log_r'] = np.log10(merged_df[f'{key}_R']+1)\n",
    "    \n",
    "    # SPX를 기준으로 데이터 processing 필요\n",
    "    merged_df[f\"{base_ticker}_vol20\"] = merged_df[f\"{base_ticker}_R\"].rolling(window=20, min_periods=1).std()\n",
    "    merged_df[f\"{base_ticker}_vol60\"] = merged_df[f\"{base_ticker}_R\"].rolling(window=60, min_periods=1).std()\n",
    "    merged_df[f\"{base_ticker}_vol20_div_vol60\"] = merged_df[f\"{base_ticker}_vol20\"] / merged_df[f\"{base_ticker}_vol60\"]\n",
    "    merged_df.loc[0, f\"{base_ticker}_vol20\"] = 0 \n",
    "    merged_df.loc[0, f\"{base_ticker}_vol60\"] = 0\n",
    "    merged_df.loc[0, f\"{base_ticker}_vol20_div_vol60\"] = 1\n",
    "    \n",
    "    for col in [f'{base_ticker}_vol20', f'{base_ticker}_vol20_div_vol60', f'VIX_close']:\n",
    "        expanding_mean = merged_df[col].expanding().mean()\n",
    "        expanding_std = merged_df[col].expanding().std()\n",
    "        merged_df[f'{col}_normalized'] = (merged_df[col] - expanding_mean) / expanding_std\n",
    "    \n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def overall_processing(windows_list_df_dict: List[Dict[str, Dict[str, pd.DataFrame]]]) -> List[Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    new_list = []\n",
    "    for each_fold in windows_list_df_dict:\n",
    "        new_elem_dict = {}\n",
    "        for key, each_split in each_fold.items():\n",
    "            new_elem_dict[key] = elem_overall_processing(each_split)\n",
    "        new_list.append(new_elem_dict)\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "processed_windows_list_df_dict_deformed = overall_processing(windows_list_df_dict_deformed)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_windows_list_df_dict_deformed[0][\"train\"].to_csv(\"./temp/processed_data0610_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  SPX  VIX\n",
      "0 2000-01-01    0   10\n",
      "1 2000-01-02    1   11\n",
      "2 2000-01-03    2   12\n",
      "3 2000-01-04    3   13\n",
      "4 2000-01-05    4   14\n",
      "5 2000-01-06    5   15\n",
      "6 2000-01-07    6   16\n",
      "7 2000-01-08    7   17\n",
      "8 2000-01-09    8   18\n",
      "9 2000-01-10    9   19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1409192/3305774918.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  each_df.rename(columns={'Price': key}, inplace=True)\n",
      "/tmp/ipykernel_1409192/3305774918.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  each_df.rename(columns={'Price': key}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "### reference code block subject to delete ###\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def elem_overall_processing(df_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    new_df_list = []\n",
    "    \n",
    "    for key, each_df in df_dict.items():\n",
    "        # Date와 Price 열만 남기고 다른 열 삭제\n",
    "        each_df = each_df[['Date', 'Price']]\n",
    "        \n",
    "        # Price 열의 이름을 key로 변경\n",
    "        each_df.rename(columns={'Price': key}, inplace=True)\n",
    "        \n",
    "        # 새로운 DataFrame 리스트에 추가\n",
    "        new_df_list.append(each_df)\n",
    "    \n",
    "    # Date 열을 기준으로 모든 DataFrame 병합\n",
    "    merged_df = new_df_list[0]\n",
    "    for df in new_df_list[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on='Date', how='outer')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# 예시 데이터 생성\n",
    "data1 = {\n",
    "    'Date': pd.date_range(start='1/1/2000', periods=10, freq='D'),\n",
    "    'Price': range(10),\n",
    "    'Open': range(10, 20),\n",
    "    'High': range(20, 30),\n",
    "    'Low': range(30, 40),\n",
    "    'Vol.': range(40, 50),\n",
    "    'Change %': range(50, 60)\n",
    "}\n",
    "data2 = {\n",
    "    'Date': pd.date_range(start='1/1/2000', periods=10, freq='D'),\n",
    "    'Price': range(10, 20),\n",
    "    'Open': range(20, 30),\n",
    "    'High': range(30, 40),\n",
    "    'Low': range(40, 50),\n",
    "    'Vol.': range(50, 60),\n",
    "    'Change %': range(60, 70)\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'SPX': pd.DataFrame(data1),\n",
    "    'VIX': pd.DataFrame(data2)\n",
    "}\n",
    "\n",
    "# 전체 처리 함수 호출\n",
    "result_df = elem_overall_processing(df_dict)\n",
    "\n",
    "# 결과 확인\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SPX</th>\n",
       "      <th>VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-01-08</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-01-09</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  SPX  VIX\n",
       "0 2000-01-01    0   10\n",
       "1 2000-01-02    1   11\n",
       "2 2000-01-03    2   12\n",
       "3 2000-01-04    3   13\n",
       "4 2000-01-05    4   14\n",
       "5 2000-01-06    5   15\n",
       "6 2000-01-07    6   16\n",
       "7 2000-01-08    7   17\n",
       "8 2000-01-09    8   18\n",
       "9 2000-01-10    9   19"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   0,    1,    2, ..., 1761, 1762, 1763]),\n",
       " array([ 252,  253,  254, ..., 2013, 2014, 2015]),\n",
       " array([ 504,  505,  506, ..., 2265, 2266, 2267]),\n",
       " array([ 756,  757,  758, ..., 2517, 2518, 2519]),\n",
       " array([1008, 1009, 1010, ..., 2769, 2770, 2771]),\n",
       " array([1260, 1261, 1262, ..., 3021, 3022, 3023]),\n",
       " array([1512, 1513, 1514, ..., 3273, 3274, 3275]),\n",
       " array([1764, 1765, 1766, ..., 3525, 3526, 3527]),\n",
       " array([2016, 2017, 2018, ..., 3777, 3778, 3779])]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sliding window by date\n",
    "\n",
    "arr = np.arange(0, 4028)\n",
    "window_size = 252*(5+1+1)\n",
    "step_size = 252\n",
    "\n",
    "def sliding_window_slicing(arr, window_size, step_size):\n",
    "    num_windows = (len(arr) - window_size) // step_size + 1\n",
    "    windows = []\n",
    "    for i in range(num_windows):\n",
    "        start = i * step_size\n",
    "        end = start + window_size\n",
    "        windows.append(arr[start:end])\n",
    "    return windows\n",
    "\n",
    "result = sliding_window_slicing(arr, window_size, step_size)\n",
    "len(result)\n",
    "result\n",
    "# for win_idx in range(0, 4028, 252):\n",
    "#     print(win_idx, win_idx + 252*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X: fold 방법으로 해야 겠다. 그리고, 5:1:1의 비율로 test하는 것으로 한다.\n",
    "\n",
    "할 것 7년 단위 sampler implement해야 한다.\n",
    "10개씩 window를 가지고 하는 방식으로,\n",
    "\n",
    "나의 방법의 우월성을 보이려면 단순하게 가장 마지막 년도에 대하여, 좋은 성능을 보이면 된다.\n",
    "data sample의 경우에 4000개 정도는 되서, 아마도 매우 작은 lora network 구현 하면 일단은 되긴 할 듯\n",
    "prompt 기반으로 finetune하면 되기야 될 듯\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
