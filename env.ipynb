{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "from typing import Optional\n",
    "import gymnasium as gym\n",
    "\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import const "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def make_env(env_id: str, rank: int, seed: int = 0) -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :return: (Callable)\n",
    "    \"\"\"\n",
    "\n",
    "    def _init() -> gym.Env:\n",
    "        env = gym.make(env_id)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "\n",
    "    set_random_seed(seed)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym.Env를 상속해서 새로운 환경을 만들어야 한다.\n",
    "class MarketEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    action space는 이용 가능한 portfolio n개에 대해서, 각각의 비율을 조절하는 것이다.\n",
    "    해당 action을 취하면 결과로 취득할 수 있는 것이 differential sharpe ratio와 \n",
    "    \n",
    "    sharpe ratio: risk-adjusted return\n",
    "    \n",
    "    첫 거래일, 바로 다음 거래일, 최종 거래일\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 lookback_T: int,\n",
    "                 asset_definition: Dict[str, str],\n",
    "                 market_df: pd.DataFrame,\n",
    "                 debug: bool = False\n",
    "                 ):\n",
    "        \n",
    "        self.debug = debug\n",
    "        \n",
    "        # some definiitions of assets\n",
    "        self.market_df: pd.DataFrame = market_df\n",
    "        self.idx2asset = {i: asset for i, asset in enumerate(asset_definition.keys())}\n",
    "        self.idx2asset[len(asset_definition)] = 'cash'\n",
    "        self.asset2idx = {asset: i for i, asset in enumerate(asset_definition.keys())}\n",
    "        self.asset2idx['cash'] = len(asset_definition)\n",
    "\n",
    "        self.lookback_T = lookback_T\n",
    "        self.business_days = len(market_df)\n",
    "        self.num_securities = len(asset_definition)\n",
    "        self.num_all_asset = self.num_securities + 1  # including cash\n",
    "        \n",
    "        # RL agent 학습을 위한 state 정보\n",
    "        \n",
    "        # time stamp 만들기\n",
    "        self.time_step = 0\n",
    "        # [S_1, S_2, ..., S_n], 나중에 debug하기 쉽도록 전체 정보에 대한 저장\n",
    "        self.overall_state = np.zeros(\n",
    "            (self.business_days, self.num_all_asset, self.lookback_T)\n",
    "        )  # to handle cash\n",
    "        \n",
    "        # portfolio 가치 산정을 위한 부분\n",
    "        # 현금 이외의 금액과 현금 금액\n",
    "        self.portfolio_ac = np.zeros((self.business_days, 2))\n",
    "        # 실 거래를 위한 quantized shares\n",
    "        self.portfolio_shares = np.zeros((self.business_days, self.num_securities))\n",
    "        # sharpe ratio 계산을 위한 부분\n",
    "        self.portfolio_return = np.zeros((self.business_days, 1))\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(self.num_all_asset, self.lookback_T),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(self.num_securities,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        # n개의 자산에 대해서 예측을 하되, cash의 경우에는 1 - \\sum_ i w_i로 계산\n",
    "        \n",
    "        ### example\n",
    "        # # Define the agent and target location; randomly chosen in `reset` and updated in `step`\n",
    "        # self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        # self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "        # # Observations are dictionaries with the agent's and the target's location.\n",
    "        # # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        # self.observation_space = gym.spaces.Dict(\n",
    "        #     {\n",
    "        #         \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "        #         \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "        #     }\n",
    "        # )\n",
    "        # # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        # self.action_space = gym.spaces.Discrete(4)\n",
    "        # # Dictionary maps the abstract actions to the directions on the grid\n",
    "        # self._action_to_direction = {\n",
    "        #     0: np.array([1, 0]),  # right\n",
    "        #     1: np.array([0, 1]),  # up\n",
    "        #     2: np.array([-1, 0]),  # left\n",
    "        #     3: np.array([0, -1]),  # down\n",
    "        # }\n",
    "        ###\n",
    "    \n",
    "    def _get_asset_columns(self, suffix: str):\n",
    "        \"\"\"\n",
    "        cash를 제외한 column들을 가져오기 위함\n",
    "        \"\"\"\n",
    "        asset_columns = [f\"{asset}_{suffix}\" for asset in self.idx2asset.values()]\n",
    "        del asset_columns[-1]\n",
    "        \n",
    "        return asset_columns\n",
    "        \n",
    "        \n",
    "    \n",
    "    def reset(self, initial_cash: float, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \"\"\"\n",
    "        여기서 overall obersevation를 잘 정의 해주도록 한다.\n",
    "        \"\"\"\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.business_start_idx =  self.time_step = self.lookback_T - 1\n",
    "        # 처음 거래 시작일, portfolio의 처음은 당연하게도 현금만 운용 가능\n",
    "        \n",
    "        # RL environment 초기화\n",
    "        asset_columns = self._get_asset_columns(\"log_r\")\n",
    "        \n",
    "        for t in range(self.lookback_T-1,self.business_days, 1):\n",
    "            # 당일 전부터 lookback_T일 전까지의 데이터를 저장\n",
    "            self.overall_state[t, 1:, 1:] = self.market_df[asset_columns][(t-(self.lookback_T-1)):t][::-1].to_numpy().T\n",
    "            \n",
    "            # 현금과 변동성 정보\n",
    "            self.overall_state[t, -1][1:3] = self.market_df[[\"SPX_vol20_normalized\", \"SPX_vol20_div_vol60_normalized\"]][t-1:t].to_numpy()\n",
    "            self.overall_state[t, -1][3:] = self.market_df[[\"VIX_close_normalized\"]][t-(self.lookback_T-3):t][::-1].to_numpy().T\n",
    "        self.overall_state[:self.lookback_T-1, -1, 0] = 1.0  # cash\n",
    "\n",
    "        # portfolio 초기화\n",
    "        # T-1부터 거래를 시작해야함, 실제로는 T일에 해당하는 거래 data를 활용해야 함\n",
    "        self.portfolio_ac[:self.lookback_T-1, 1] = initial_cash\n",
    "        \n",
    "        ### example\n",
    "        # Choose the agent's location uniformly at random\n",
    "        # self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "        # # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        # self._target_location = self._agent_location\n",
    "        # while np.array_equal(self._target_location, self._agent_location):\n",
    "        #     self._target_location = self.np_random.integers(\n",
    "        #         0, self.size, size=2, dtype=int\n",
    "        #     )\n",
    "        ### example\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "        return {\n",
    "            \"state_t\": self.overall_state[self.time_step],\n",
    "        }\n",
    "        # agent는 특정 시점일때의 정보에만 접근가능 하도록 하기\n",
    "        \n",
    "    \n",
    "    def _get_info(self):\n",
    "        \"\"\"\n",
    "        여기는 internal 분석용으로 쓰면 될 듯 하다\n",
    "        여기서는 portfolio의 가치와 shares 반환하면 검산할 때 편할 듯\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \n",
    "        }\n",
    "    \n",
    "    def _compute_clip(self, action: np.ndarray):\n",
    "        # action model이 예측한 raw weights\n",
    "        \n",
    "        columns = self._get_asset_columns(\"close\")\n",
    "        current_asset_prices = self.market_df[columns][self.time_step:self.time_step+1].to_numpy()\n",
    "        current_securities_sum = np.sum(self.portfolio_shares[self.time_step-1] * current_asset_prices)\n",
    "        current_portfolio_sum = current_securities_sum + self.portfolio_ac[self.time_step-1, 1]\n",
    "        \n",
    "        self.portfolio_shares[self.time_step, :] = np.floor((action * current_portfolio_sum) / current_asset_prices)\n",
    "        \n",
    "        self.portfolio_ac[self.time_step, 0] = np.sum(self.portfolio_shares[self.time_step, :] * current_asset_prices)  # 새로운 비중으로 계산된 securities\n",
    "        self.portfolio_ac[self.time_step, 1] = current_cash = current_portfolio_sum - np.sum(self.portfolio_shares[self.time_step, :] * current_asset_prices)  # 새로운 비중으로 계산된 cash\n",
    "        self.portfolio_return[self.time_step] = (self.portfolio_ac[self.time_step, 0] - self.portfolio_ac[self.time_step-1, 0]) / self.portfolio_ac[self.time_step-1, 0]\n",
    "        \n",
    "        self.overall_state[self.time_step, :self.num_securities, 0] = (self.portfolio_shares[self.time_step, :] * current_asset_prices) / current_portfolio_sum\n",
    "        self.overall_state[self.time_step, -1, 0] = current_cash / current_portfolio_sum\n",
    "    \n",
    "    def _compute_reward(self):\n",
    "        \"\"\"\n",
    "        Differntial Sharpe Ratio 계산\n",
    "        \n",
    "        \"\"\"\n",
    "        # 거래 시작일 부터 portfolio의 return을 계산해야 함\n",
    "        \n",
    "        A_t1 = self.portfolio_return[self.business_start_idx:self.time_step].mean()\n",
    "        delta_A_t = self.portfolio_return[self.time_step] - A_t1\n",
    "        A_t = self.portfolio_ac[self.business_start_idx:self.time_step+1, 0].mean()\n",
    "\n",
    "        B_t1 = (self.portfolio_ac[self.business_start_idx:self.time_step, 0] ** 2).mean()\n",
    "        delta_B_t = (self.portfolio_return[self.time_step] ** 2) - (B_t1)\n",
    "        B_t = (self.portfolio_ac[self.business_start_idx:self.time_step+1, 0] ** 2).mean()\n",
    "\n",
    "        diff_sharpe_ratio = ((B_t1 * delta_A_t - (1/2)*A_t1*delta_B_t)/(B_t1 - A_t1**2)**(3/2)) \n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action은 각 portfolio 배분 비율로 하면 될듯 하다.\n",
    "        \n",
    "        action을 취했을 때, 어떠한 결과를 얻어야 하는가?\n",
    "        \n",
    "        PortVal_t = \\sum P_{i,t} \\times shares_{i, t-1} + cash_{t-1}\n",
    "        \n",
    "        shares \n",
    "        \"\"\"\n",
    "        self.time_step += 1\n",
    "        \n",
    "        self._compute_clip(action)  # 자산 가치 계산\n",
    "        \n",
    "\n",
    "        # An environment is completed if and only if the agent has reached the target\n",
    "        terminated = True if self.time_step == self.business_days else False\n",
    "        truncated = False\n",
    "        \n",
    "        reward = 1 if terminated else 0  # the agent is only reached at the end of the episode\n",
    "        observation = self._get_obs()\n",
    "        \n",
    "        info = self._get_info()\n",
    "\n",
    "        # # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        # direction = self._action_to_direction[action]\n",
    "        # # We use `np.clip` to make sure we don't leave the grid bounds\n",
    "        # self._agent_location = np.clip(\n",
    "        #     self._agent_location + direction, 0, self.size - 1\n",
    "        # )\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1259, 12, 60)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env snippet\n",
    "\"\"\"\n",
    "state를 초기화 하기 위한 함수\n",
    "\n",
    "T-1 거래일 부터 정보들을 기입하기로 한다.\n",
    "그 이전의 정보에 대해서는 접근할 수 없다고 생각한다.\n",
    "\"\"\"\n",
    "test_df = pd.read_csv('./temp/processed_data0610_test.csv')\n",
    "\n",
    "test_env = MarketEnv(\n",
    "    lookback_T=const.LOOKBACK_T,\n",
    "    asset_definition=const.SNP_INDICES_ASSETS,\n",
    "    market_df=test_df\n",
    ")\n",
    "\n",
    "test_env.reset(initial_cash=100_000)\n",
    "\n",
    "test_env.market_df.loc[test_env.time_step, :]\n",
    "# test_env.\n",
    "\n",
    "test_action = np.ones(test_env.num_securities) / test_env.num_securities\n",
    "\n",
    "test_env.overall_state.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.overall_state[test_env.lookback_T-2, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_env.lookback_T - 1\n",
    "\n",
    "asset_columns = [f\"{asset}_log_r\" for asset in test_env.idx2asset.values()]\n",
    "del asset_columns[-1]\n",
    "test_env.overall_state[t, :len(asset_columns), 1:] = test_env.market_df[asset_columns][(t-(test_env.lookback_T-1)):t][::-1].to_numpy().T  # 거래일 바로 전날까지의 데이터를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.market_df[[\"SPX_log_r\", \"SPLRCS_log_r\"]][10:100][::-1].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26., 67., 45., 60., 37., 20., 22., 24., 34., 29., 57.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = test_env._get_asset_columns(\"close\")\n",
    "current_asset_prices = test_env.market_df[columns][t:t+1].to_numpy()\n",
    "np.floor(test_action * 100_000 / current_asset_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [inf],\n",
       "       ...,\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.portfolio_return[t-2:]\n",
    "# test_env.portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3248403/4027241158.py:170: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  self.portfolio_return[self.time_step] = (self.portfolio_ac[self.time_step, 0] - self.portfolio_ac[self.time_step-1, 0]) / self.portfolio_ac[self.time_step-1, 0]\n"
     ]
    }
   ],
   "source": [
    "test_action = np.ones(test_env.num_securities) / test_env.num_securities\n",
    "test_env._compute_clip(test_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.089648 , 0.0901016, 0.09027  , 0.090636 , 0.0907166, 0.087982 ,\n",
       "       0.0901164, 0.0896256, 0.090882 , 0.0895752, 0.0907383])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.overall_state[test_env.time_step -1, :test_env.num_securities, 0]\n",
    "test_env.portfolio_ac[test_env.time_step]\n",
    "\n",
    "test_env.overall_state[test_env.time_step, :test_env.num_securities, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Date', 'SPX_close', 'SPLRCT_close', 'SPLRCL_close',\n",
       "       'SPLRCM_close', 'SPLRCREC_close', 'SPLRCS_close', 'SPSY_close',\n",
       "       'SPNY_close', 'SPXHC_close', 'SPLRCD_close', 'SPLRCI_close',\n",
       "       'SPLRCU_close', 'VIX_close', 'SPX_R', 'SPX_log_r', 'SPLRCT_R',\n",
       "       'SPLRCT_log_r', 'SPLRCL_R', 'SPLRCL_log_r', 'SPLRCM_R', 'SPLRCM_log_r',\n",
       "       'SPLRCREC_R', 'SPLRCREC_log_r', 'SPLRCS_R', 'SPLRCS_log_r', 'SPSY_R',\n",
       "       'SPSY_log_r', 'SPNY_R', 'SPNY_log_r', 'SPXHC_R', 'SPXHC_log_r',\n",
       "       'SPLRCD_R', 'SPLRCD_log_r', 'SPLRCI_R', 'SPLRCI_log_r', 'SPLRCU_R',\n",
       "       'SPLRCU_log_r', 'SPX_vol20', 'SPX_vol60', 'SPX_vol20_div_vol60',\n",
       "       'SPX_vol20_normalized', 'SPX_vol20_div_vol60_normalized',\n",
       "       'VIX_close_normalized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.market_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PPO를 쓰기 위한 multiprocess 환경 설정\n",
    "\n",
    "# env_id = \"CartPole-v1\"\n",
    "# num_cpu = 4  # Number of processes to use\n",
    "# # Create the vectorized environment\n",
    "# env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
