{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "from typing import Optional\n",
    "import gymnasium as gym\n",
    "\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import const "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def make_env(env_id: str, rank: int, seed: int = 0) -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :return: (Callable)\n",
    "    \"\"\"\n",
    "\n",
    "    def _init() -> gym.Env:\n",
    "        env = gym.make(env_id)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "\n",
    "    set_random_seed(seed)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# gym.Env를 상속해서 새로운 환경을 만들어야 한다.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGridWorldEnv\u001b[39;00m(\u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# The size of the square grid\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# gym.Env를 상속해서 새로운 환경을 만들어야 한다.\n",
    "class GridWorldEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    action space는 이용 가능한 portfolio n개에 대해서, 각각의 비율을 조절하는 것이다.\n",
    "    해당 action을 취하면 결과로 취득할 수 있는 것이 differential sharpe ratio와 \n",
    "    \n",
    "    sharpe ratio: risk-adjusted return\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 lookback_T: int,\n",
    "                 asset_definition: Dict[str, str],\n",
    "                 market_df: pd.DataFrame,\n",
    "                 ):\n",
    "        \n",
    "        # some definiitions of assets\n",
    "        self.idx2asset = {i: asset for i, asset in enumerate(asset_definition.keys())}\n",
    "        self.lookpack_T = lookback_T # TODO\n",
    "        self.business_days = len(market_df)\n",
    "        self.num_securities = len(asset_definition)  # TODO\n",
    "        self.num_all_asset = self.num_securities + 1  # including cash\n",
    "        \n",
    "        # [S_1, S_2, ..., S_n], 나중에 debug하기 쉽도록 전체 정보에 대한 저장\n",
    "        self.overall_state = torch.zeros(\n",
    "            (self.business_days, self.num_all_asset, self.lookback_T)\n",
    "        )  # to handle cash\n",
    "        \n",
    "        self.portfolio = torch.zeros((self.business_days, self.num_all_asset))\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(self.num_all_asset, self.lookback_T),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(self.num_all_asset,),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        ### example\n",
    "        # Define the agent and target location; randomly chosen in `reset` and updated in `step`\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "        ###\n",
    "        \n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \"\"\"\n",
    "        여기서 overall obersevation를 잘 정의 해주도록 한다.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    \n",
    "    def _get_info(self):\n",
    "        \"\"\"\n",
    "        여기는 internal 분석용으로 쓰면 될 듯 하다\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action은 각 portfolio 배분 비율로 하면 될듯 하다.\n",
    "        \n",
    "        action을 취했을 때, 어떠한 결과를 얻어야 하는가?\n",
    "        \n",
    "        \"\"\"\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid bounds\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # An environment is completed if and only if the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        truncated = False\n",
    "        \n",
    "        reward = 1 if terminated else 0  # the agent is only reached at the end of the episode\n",
    "        observation = self._get_obs()\n",
    "        \n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO를 쓰기 위한 multiprocess 환경 설정\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "num_cpu = 4  # Number of processes to use\n",
    "# Create the vectorized environment\n",
    "env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
