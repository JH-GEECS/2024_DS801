{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO agent 학습용 scratch codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['MKL_SERVICE_FORCE_INTEL'] = '1'\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "import const\n",
    "from env import MarketEnv\n",
    "\n",
    "from typing import List, Dict, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(market_df: pd.DataFrame, \n",
    "             asset_definition: dict, \n",
    "             initial_cash: float, \n",
    "             lookback_T: int, \n",
    "             sharpe_eta: float,\n",
    "             deterministic: bool = False, \n",
    "             debug: bool = False) -> Callable[[], gym.Env]:\n",
    "    \"\"\"\n",
    "    SubprocVecEnv 사용을 위해 환경을 생성하는 함수\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = MarketEnv(\n",
    "            lookback_T=lookback_T,\n",
    "            sharpe_eta=sharpe_eta,\n",
    "            asset_definition=asset_definition,\n",
    "            market_df=market_df,\n",
    "            initial_cash=initial_cash,\n",
    "            deterministic=deterministic,\n",
    "            debug=debug\n",
    "        )\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def get_ranged_schedule_fn(lr_range):\n",
    "    start_lr, end_lr = lr_range\n",
    "    def lr_schedule(progress_remaining):\n",
    "        return end_lr + (start_lr - end_lr) * progress_remaining\n",
    "    return lr_schedule\n",
    "\n",
    "def fold_data_load(root_dir: str):\n",
    "    dataset_list: List[Dict[str, pd.DataFrame]] = []\n",
    "    \n",
    "    folds = os.listdir(root_dir)\n",
    "\n",
    "    for fold in folds:\n",
    "        fold_path = os.path.join(root_dir, fold)\n",
    "        dataset = {}\n",
    "        for data_type in const.SPLIT:\n",
    "            data = pd.read_csv(os.path.join(fold_path, f\"{data_type}_processed.csv\"))\n",
    "            dataset[data_type] = data\n",
    "        dataset_list.append(dataset)\n",
    "    \n",
    "    return dataset_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david3684/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Trying to log data to tensorboard but tensorboard is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 100\u001b[0m\n\u001b[1;32m     83\u001b[0m     model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     85\u001b[0m         train_env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m         tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_tensorboard/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# 학습 진행\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# validation 평가 후 best model 확인\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# EvalCallback이 best_model을 ./logs/best_model.zip에 저장했을 것이므로\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# 여기서 validation 성능 확인 가능 (로그파일 분석 또는 callback 개선)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# 예: best_model.zip 로드 후 validation 평가\u001b[39;00m\n\u001b[1;32m    112\u001b[0m val_model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs/best_model.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:310\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[1;32m    308\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 310\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:431\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Configure logger's outputs if no logger was passed\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_logger:\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Create eval callback if needed\u001b[39;00m\n\u001b[1;32m    434\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_callback(callback, progress_bar)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/utils.py:201\u001b[0m, in \u001b[0;36mconfigure_logger\u001b[0;34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    198\u001b[0m save_path, format_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to log data to tensorboard but tensorboard is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     latest_run_id \u001b[38;5;241m=\u001b[39m get_latest_run_id(tensorboard_log, tb_log_name)\n",
      "\u001b[0;31mImportError\u001b[0m: Trying to log data to tensorboard but tensorboard is not installed."
     ]
    }
   ],
   "source": [
    "data_root = \"./data/processed\"\n",
    "data_list = fold_data_load(data_root)\n",
    "\n",
    "n_envs = 10\n",
    "n_steps = 252 * 3 * n_envs  # rollout buffer size\n",
    "total_timesteps = n_steps\n",
    "SEEDS = [1000, 2000, 3000, 4000, 5000]\n",
    "\n",
    "ppo_hyperparams = const.PPOEnvConst()\n",
    "data_definition = const.AssetConst()\n",
    "\n",
    "for idx, each_fold in enumerate(data_list):\n",
    "    \n",
    "    train_data = each_fold[\"train\"]\n",
    "    valid_data = each_fold[\"val\"]\n",
    "    test_data = each_fold[\"test\"]\n",
    "\n",
    "    # 초기 정책 로딩 (이전 윈도우에서 잘된 정책, 첫 윈도우는 랜덤)\n",
    "    # policy_path = f\"ppo_policy_{start_year-1}.zip\" # 이전 윈도우에서 잘 된 정책 경로\n",
    "    # 만약 첫 윈도우이면 policy_path가 없을 수도 있음, 이 경우 None\n",
    "    # seed agents를 위한 loop\n",
    "    best_validation_reward = -np.inf\n",
    "    best_agent_path = None\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        # 병렬 환경 생성 (학습용)\n",
    "        def train_env_factory():\n",
    "            return make_env(\n",
    "                market_df=train_data,\n",
    "                asset_definition=data_definition.SNP_INDICES_ASSETS,\n",
    "                initial_cash=data_definition.initial_cash,\n",
    "                lookback_T=data_definition.LOOKBACK_T,\n",
    "                sharpe_eta=data_definition.SHARPE_ETA,\n",
    "                deterministic=False,\n",
    "                debug=False\n",
    "            )\n",
    "        \n",
    "        train_env = SubprocVecEnv([train_env_factory() for _ in range(n_envs)])\n",
    "\n",
    "        # validation 환경 (evaluation용)\n",
    "        def val_env_factory():\n",
    "            return make_env(\n",
    "                market_df=valid_data,\n",
    "                asset_definition=data_definition.SNP_INDICES_ASSETS,\n",
    "                initial_cash=data_definition.initial_cash,\n",
    "                lookback_T=data_definition.LOOKBACK_T,\n",
    "                sharpe_eta=data_definition.SHARPE_ETA,\n",
    "                deterministic=True,\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "        val_env = make_env(\n",
    "            market_df=valid_data,\n",
    "            asset_definition=data_definition.SNP_INDICES_ASSETS,\n",
    "            initial_cash=data_definition.initial_cash,\n",
    "            lookback_T=data_definition.LOOKBACK_T,\n",
    "            sharpe_eta=data_definition.SHARPE_ETA,\n",
    "            deterministic=True,\n",
    "            debug=False\n",
    "        )()\n",
    "\n",
    "        # EvalCallback 설정\n",
    "        eval_callback = EvalCallback(\n",
    "            val_env,\n",
    "            best_model_save_path='./logs/',\n",
    "            log_path='./logs/',\n",
    "            eval_freq=n_steps,  # 주기적으로 validation 평가\n",
    "            deterministic=True,\n",
    "            render=False\n",
    "        )\n",
    "\n",
    "        # 이전 윈도우의 best agent를 초기 파라미터로 로드할지 여부\n",
    "        # 여기서는 예시로 이전 window best_agent_path가 있다면 로드\n",
    "        if best_agent_path is not None:\n",
    "            model = PPO.load(\n",
    "                best_agent_path,\n",
    "                env=train_env,\n",
    "                device='auto'\n",
    "            )\n",
    "            model.set_parameters(model.get_parameters())  # 현재 PPO에 파라미터 셋팅\n",
    "        else:\n",
    "            # 처음 윈도우면 새로운 PPO\n",
    "            model = PPO(\n",
    "                \"MlpPolicy\",\n",
    "                train_env,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=ppo_hyperparams.BATCH_SIZE,\n",
    "                n_epochs=ppo_hyperparams.N_EPOCHS,\n",
    "                gamma=ppo_hyperparams.GAMMA,\n",
    "                gae_lambda=ppo_hyperparams.GAE_LAMBDA,\n",
    "                clip_range=ppo_hyperparams.CLIP_RANGE,\n",
    "                learning_rate=get_ranged_schedule_fn((ppo_hyperparams.LEARNING_RATE_START, ppo_hyperparams.LEARNING_RATE_END)),\n",
    "                policy_kwargs=ppo_hyperparams.PPOArch,\n",
    "                verbose=1,\n",
    "                seed=seed,\n",
    "                tensorboard_log=\"./ppo_tensorboard/\"\n",
    "            )\n",
    "        \n",
    "        # 학습 진행\n",
    "        model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=eval_callback\n",
    "        )\n",
    "\n",
    "        # validation 평가 후 best model 확인\n",
    "        # EvalCallback이 best_model을 ./logs/best_model.zip에 저장했을 것이므로\n",
    "        # 여기서 validation 성능 확인 가능 (로그파일 분석 또는 callback 개선)\n",
    "        # 간단히 여기서는 eval_callback에서 나온 best_mean_reward 비교\n",
    "        # 실무에서는 eval_callback을 커스텀하거나, 저장된 모델로 다시 평가하여 reward를 계산한 뒤 비교\n",
    "        \n",
    "        # 예: best_model.zip 로드 후 validation 평가\n",
    "        val_model = PPO.load(\"./logs/best_model.zip\")\n",
    "        # validation 평가\n",
    "        # 간단히 rollout해서 평균 reward 계산 예시\n",
    "        val_episodes = 10\n",
    "        episode_rewards = []\n",
    "        for _ in range(val_episodes):\n",
    "            obs, info = val_env.reset()\n",
    "            done = False\n",
    "            total_rew = 0\n",
    "            while True:\n",
    "                action, _states = val_model.predict(obs, deterministic=True)\n",
    "                obs, rew, terminated, truncated, info = val_env.step(action)\n",
    "                total_rew += rew\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            episode_rewards.append(total_rew)\n",
    "        mean_val_reward = np.mean(episode_rewards)\n",
    "\n",
    "        if mean_val_reward > best_validation_reward:\n",
    "            best_validation_reward = mean_val_reward\n",
    "            best_agent_path = f\"./logs/best_window_{idx}_{seed}.zip\"\n",
    "            val_model.save(best_agent_path)\n",
    "\n",
    "        train_env.close()\n",
    "        del train_env\n",
    "        val_env.close()\n",
    "        del val_env\n",
    "\n",
    "    # best_agent_path 가 현재 윈도우의 best policy\n",
    "    # 다음 윈도우 학습시 초기값으로 이용 가능\n",
    "    # 테스트 데이터로 backtest 수행도 가능\n",
    "    test_env = make_env(\n",
    "        market_df=test_data,\n",
    "        asset_definition=data_definition.SNP_INDICES_ASSETS,\n",
    "        initial_cash=data_definition.initial_cash,\n",
    "        lookback_T=data_definition.LOOKBACK_T,\n",
    "        sharpe_eta=data_definition.SHARPE_ETA,\n",
    "        deterministic=True,\n",
    "        debug=False\n",
    "    )()\n",
    "\n",
    "    best_model = PPO.load(best_agent_path)\n",
    "    obs, info = test_env.reset()\n",
    "    done = False\n",
    "    test_rewards = []\n",
    "    while True:\n",
    "        action, _states = best_model.predict(obs, deterministic=True)\n",
    "        obs, rew, terminated, truncated, info = test_env.step(action)\n",
    "        test_rewards.append(rew)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    mean_test_reward = np.mean(test_rewards)\n",
    "    print(f\"Window {idx}, Best validation reward: {best_validation_reward}, Test reward: {mean_test_reward}\")\n",
    "    test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
